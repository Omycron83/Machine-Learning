@book{Bellman1972,
   abstract = {"A Rand Corporation research study."},
   author = {Richard Bellman},
   isbn = {069107951X},
   pages = {342},
   publisher = {Princeton University Press},
   title = {Dynamic programming.},
   year = {1972},
}
@book{,
   abstract = {Second edition. "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms"--Provided by publisher. Introduction. Reinforcement Learning -- Examples -- Elements of Reinforcement Learning -- Limitations and Scope -- An Extended Example: Tic-Tac-Toe -- Summary -- Early History of Reinforcement Learning -- Tabular Solution Methods. Multi-armed Bandits. A k-armed Bandit Problem -- Action-value Methods -- The 10-armed Testbed -- Incremental Implementation -- Tracking a Nonstationary Problem -- Optimistic Initial Values -- Upper-Confidence-Bound Action Selection -- Gradient Bandit Algorithms -- Associative Search (Contextual Bandits) -- Summary -- Finite Markov Decision Processes -- The Agent-Environment Interface -- Goals and Rewards -- Returns and Episodes -- Unified Notation for Episodic and Continuing Tasks -- Policies and Value Functions -- Optimal Policies and Optimal Value Functions -- Optimality and Approximation -- Summary -- Dynamic Programming. Policy Evaluation (Prediction) -- Policy Improvement -- Policy Iteration -- Value Iteration -- Asynchronous Dynamic Programming -- Generalized Policy Iteration -- Efficiency of Dynamic Programming -- Summary -- Monte Carlo Methods. Monte Carlo Prediction -- Monte Carlo Estimation of Action Values -- Monte Carlo Control -- Monte Carlo Control without Exploring Starts -- Off-policy Prediction via Importance Sampling -- Incremental Implementation -- Off-policy Monte Carlo Control -- *Discounting-aware Importance Sampling -- *Per-decision Importance Sampling -- Summary -- Temporal-Difference Learning. TD Prediction -- Advantages of TD Prediction Methods -- Optimality of TD(0) -- Sarsa: On-policy TD Control -- Q-learning: Off-policy TD Control -- Expected Sarsa -- Maximization Bias and Double Learning Games, Afterstates, and Other Special Cases -- Summary -- n-step Bootstrapping. n-step TD Prediction -- n-step Sarsa -- n-step Off-policy Learning -- *Per-decision Methods with Control Variates -- Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm -- *A Unifying Algorithm: n-step Q(u) -- Summary -- Planning and Learning with Tabular Methods. Models and Planning -- Dyna: Integrated Planning, Acting, and Learning -- When the Model Is Wrong -- Prioritized Sweeping -- Expected vs. Sample Updates -- Trajectory Sampling -- Real-time Dynamic Programming -- Planning at Decision Time -- Heuristic Search -- Rollout Algorithms -- Monte Carlo Tree Search -- Summary of the Chapter -- Summary of Part I: Dimensions -- Approximate Solution Methods. On-policy Prediction with Approximation. Value-function Approximation -- The Prediction Objective (VE) Stochastic-gradient and Semi-gradient Methods -- Linear Methods -- Feature Construction for Linear Methods -- Polynomials -- Fourier Basis -- Coarse Coding -- Tile Coding -- Radial Basis Functions -- Selecting Step-Size Parameters Manually -- Nonlinear Function Approximation: Artificial Neural Networks -- Least-Squares TD -- Memory-based Function Approximation -- Kernel-based Function Approximation -- Looking Deeper at On-policy Learning: Interest and Emphasis -- Summary -- On-policy Control with Approximation. Episodic Semi-gradient Control -- Semi-gradient n-step Sarsa -- Average Reward: A New Problem Setting for Continuing Tasks -- Deprecating the Discounted Setting -- Differential Semi-gradient n-step Sarsa -- Summary -- *Off-policy Methods with Approximation. Semi-gradient Methods -- Examples of Off-policy Divergence The Deadly Triad -- Linear Value-function Geometry -- Gradient Descent in the Bellman Error -- The Bellman Error is Not Learnable -- Gradient-TD Methods -- Emphatic-TD Methods -- Reducing Variance -- Summary -- Eligibility Traces. The A-return -- TD(A) -- n-step Truncated A-return Methods -- Redoing Updates: Online A-return Algorithm -- True Online TD(A) -- *Dutch Traces in Monte Carlo Learning -- Sarsa(A) -- Variable A and ry -- Off-policy Traces with Control Variates -- Watkins's Q(A) to Tree-Backup(A) -- Stable Off-policy Methods with Traces -- Implementation Issues -- Conclusions -- Policy Gradient Methods. Policy Approximation and its Advantages -- The Policy Gradient Theorem -- REINFORCE: Monte Carlo Policy Gradient -- REINFORCE with Baseline -- Actor-Critic Methods Policy Gradient for Continuing Problems -- Policy Parameterization for Continuous Actions -- Summary -- Looking Deeper. Psychology. Prediction and Control -- Classical Conditioning -- Blocking and Higher-order Conditioning -- The Rescorla-Wagner Model -- The TD Model -- TD Model Simulations -- Instrumental Conditioning -- Delayed Reinforcement -- Cognitive Maps -- Habitual and Goal-directed Behavior -- Summary -- Neuroscience -- Neuroscience Basics -- Reward Signals, Reinforcement Signals, Values, and Prediction Errors -- The Reward Prediction Error Hypothesis -- Dopamine -- Experimental Support for the Reward Prediction Error Hypothesis -- TD Error/Dopamine Correspondence -- Neural Actor-Critic -- Actor and Critic Learning Rules -- Hedonistic Neurons -- Collective Reinforcement Learning -- Model-based Methods in the Brain Addiction -- Summary -- Applications and Case Studies. TD-Gammon -- Samuel's Checkers Player -- Watson's Daily-Double Wagering -- Optimizing Memory Control -- Human-level Video Game Play -- Mastering the Game of Go -- AlphaGo -- AlphaGo Zero -- Personalized Web Services -- Thermal Soaring -- Frontiers. General Value Functions and Auxiliary Tasks -- Temporal Abstraction via Options -- Observations and State -- Designing Reward Signals -- Remaining Issues -- Experimental Support for the Reward Prediction Error Hypothesis.},
   author = {Richard S. Sutton and Andrew G. Barto},
   isbn = {9780262039246},
   pages = {526},
   title = {Reinforcement learning : an introduction},
}
@misc{,
   abstract = {B e n j a m i n B l a n k e r t z u n d V e r a R ö h r A l g o r i t h m e n u n d D a t e n s t r u k t u r e n},
   author = {Benjamin Blankertz and Vera Röhr},
   title = {Algorithmen und Datenstrukturen},
}
@article{,
   abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
   author = {Gabriel Dulac-Arnold and Nir Levine and Daniel J. Mankowitz and Jerry Li and Cosmin Paduraru and Sven Gowal and Todd Hester},
   doi = {10.1007/s10994-021-05961-4},
   issn = {15730565},
   issue = {9},
   journal = {Machine Learning},
   keywords = {Applied reinforcement learning,Real-world,Reinforcement learning},
   month = {9},
   pages = {2419-2468},
   publisher = {Springer},
   title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
   volume = {110},
   year = {2021},
}
@article{Prudencio2023,
   abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks&#x2019; properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
   author = {Rafael Figueiredo Prudencio and Marcos R.O.A. Maximo and Esther Luna Colombini},
   doi = {10.1109/TNNLS.2023.3250269},
   issn = {21622388},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Batch reinforcement learning (RL),Behavioral sciences,Benchmark testing,Games,Markov processes,RL,Taxonomy,Training,Uncertainty,deep learning (DL),offline RL},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems},
   year = {2023},
}
@article{Nakamoto2023,
   abstract = {A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that offline RL algorithms that learn such calibrated value functions lead to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL) for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL},
   author = {Mitsuhiko Nakamoto and Yuexiang Zhai and Anikait Singh and Max Sobol Mark and Yi Ma and Chelsea Finn and Aviral Kumar and Sergey Levine},
   month = {3},
   title = {Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning},
   url = {http://arxiv.org/abs/2303.05479},
   year = {2023},
}
@article{Luo2023,
   abstract = {Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.},
   author = {Yicheng Luo and Jackie Kay and Edward Grefenstette and Marc Peter Deisenroth},
   month = {3},
   title = {Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
   url = {http://arxiv.org/abs/2303.17396},
   year = {2023},
}
@article{Levine2020,
   abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
   author = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
   month = {5},
   title = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
   url = {http://arxiv.org/abs/2005.01643},
   year = {2020},
}
@article{Zhuang2023,
   abstract = {Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to the overestimation of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we get a surprising finding that some online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark indicate this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.},
   author = {Zifeng Zhuang and Kun Lei and Jinxin Liu and Donglin Wang and Yilang Guo},
   month = {2},
   title = {Behavior Proximal Policy Optimization},
   url = {http://arxiv.org/abs/2302.11312},
   year = {2023},
}
@misc{,
   author = {Sergey Levine},
   title = {Policy Gradients CS 285},
}
@article{Zheng2022,
   abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
   author = {Qinqing Zheng and Amy Zhang and Aditya Grover},
   month = {2},
   title = {Online Decision Transformer},
   url = {http://arxiv.org/abs/2202.05607},
   year = {2022},
}
@article{Li2023,
   abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
   author = {Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
   month = {1},
   title = {A Survey on Transformers in Reinforcement Learning},
   url = {http://arxiv.org/abs/2301.03044},
   year = {2023},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {6},
   title = {Attention Is All You Need},
   url = {http://arxiv.org/abs/1706.03762},
   year = {2017},
}
@article{Mathieu2023,
   abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.},
   author = {Michaël Mathieu and Sherjil Ozair and Srivatsan Srinivasan and Caglar Gulcehre and Shangtong Zhang and Ray Jiang and Tom Le Paine and Richard Powell and Konrad Żołna and Julian Schrittwieser and David Choi and Petko Georgiev and Daniel Toyama and Aja Huang and Roman Ring and Igor Babuschkin and Timo Ewalds and Mahyar Bordbar and Sarah Henderson and Sergio Gómez Colmenarejo and Aäron van den Oord and Wojciech Marian Czarnecki and Nando de Freitas and Oriol Vinyals},
   month = {8},
   title = {AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning},
   url = {http://arxiv.org/abs/2308.03526},
   year = {2023},
}
@article{Vinyals2019,
   abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
   author = {Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Michaël Mathieu and Andrew Dudzik and Junyoung Chung and David H. Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and Laurent Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander S. Vezhnevets and Rémi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom L. Paine and Caglar Gulcehre and Ziyu Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario Wünsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
   doi = {10.1038/s41586-019-1724-z},
   issn = {14764687},
   issue = {7782},
   journal = {Nature},
   month = {11},
   pages = {350-354},
   pmid = {31666705},
   publisher = {Nature Publishing Group},
   title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
   volume = {575},
   year = {2019},
}
@misc{,
   author = {Aston Zhang and Zachary C Lipton and M U Li and Alexander J Smola},
   title = {Dive into Deep Learning},
}
@misc{,
   author = {Yann Lecun and L Eon Bottou and Yoshua Bengio and Patrick Haaner Abstract|},
   title = {Gradient-Based Learning Applied to Document Recognition},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {http://arxiv.org/abs/1409.0473},
   year = {2014},
}
@article{Ba2016,
   abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
   author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   month = {7},
   title = {Layer Normalization},
   url = {http://arxiv.org/abs/1607.06450},
   year = {2016},
}
@article{Chen2021,
   abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
   author = {Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
   month = {6},
   title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
   url = {http://arxiv.org/abs/2106.01345},
   year = {2021},
}
@article{Zheng2022,
   abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
   author = {Qinqing Zheng and Amy Zhang and Aditya Grover},
   month = {2},
   title = {Online Decision Transformer},
   url = {http://arxiv.org/abs/2202.05607},
   year = {2022},
}
@article{Zhang2023,
   abstract = {Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.},
   author = {Tianjun Zhang and Fangchen Liu and Justin Wong and Pieter Abbeel and Joseph E. Gonzalez},
   month = {2},
   title = {The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
   url = {http://arxiv.org/abs/2302.05206},
   year = {2023},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {6},
   title = {Attention Is All You Need},
   url = {http://arxiv.org/abs/1706.03762},
   year = {2017},
}
@article{Prudencio2023,
   abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks&#x2019; properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
   author = {Rafael Figueiredo Prudencio and Marcos R.O.A. Maximo and Esther Luna Colombini},
   doi = {10.1109/TNNLS.2023.3250269},
   issn = {21622388},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Batch reinforcement learning (RL),Behavioral sciences,Benchmark testing,Games,Markov processes,RL,Taxonomy,Training,Uncertainty,deep learning (DL),offline RL},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems},
   year = {2023},
}
@article{Mathieu2023,
   abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.},
   author = {Michaël Mathieu and Sherjil Ozair and Srivatsan Srinivasan and Caglar Gulcehre and Shangtong Zhang and Ray Jiang and Tom Le Paine and Richard Powell and Konrad Żołna and Julian Schrittwieser and David Choi and Petko Georgiev and Daniel Toyama and Aja Huang and Roman Ring and Igor Babuschkin and Timo Ewalds and Mahyar Bordbar and Sarah Henderson and Sergio Gómez Colmenarejo and Aäron van den Oord and Wojciech Marian Czarnecki and Nando de Freitas and Oriol Vinyals},
   month = {8},
   title = {AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning},
   url = {http://arxiv.org/abs/2308.03526},
   year = {2023},
}
@article{Li2023,
   abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
   author = {Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
   month = {1},
   title = {A Survey on Transformers in Reinforcement Learning},
   url = {http://arxiv.org/abs/2301.03044},
   year = {2023},
}
@article{Tay2022,
   abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former"models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored "X-former"models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
   author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
   doi = {10.1145/3530811},
   issn = {15577341},
   issue = {6},
   journal = {ACM Computing Surveys},
   keywords = {Transformers,attention,deep learning,neural networks},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Efficient Transformers: A Survey},
   volume = {55},
   year = {2022},
}
@article{Zhuang2023,
   abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
   author = {Bohan Zhuang and Jing Liu and Zizheng Pan and Haoyu He and Yuetian Weng and Chunhua Shen},
   month = {2},
   title = {A Survey on Efficient Training of Transformers},
   url = {http://arxiv.org/abs/2302.01107},
   year = {2023},
}
@article{Zhuang2023,
   abstract = {Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to the overestimation of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we get a surprising finding that some online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark indicate this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.},
   author = {Zifeng Zhuang and Kun Lei and Jinxin Liu and Donglin Wang and Yilang Guo},
   month = {2},
   title = {Behavior Proximal Policy Optimization},
   url = {http://arxiv.org/abs/2302.11312},
   year = {2023},
}
@misc{,
   author = {Sergey Levine},
   title = {Policy Gradients CS 285},
}
@misc{,
   title = {An Evaluation of Transformer Variants | dalle-mini-Weights & Biases https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants-VmlldzoxNjk4MTIw An Evaluation of Transformer Variants Training of different transformer variants for text-to-image generation with DALL-E-mini},
   url = {https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants--VmlldzoxNjk4MTIw},
}
@article{Dosovitskiy2020,
   abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
   author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
   month = {10},
   title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
   url = {http://arxiv.org/abs/2010.11929},
   year = {2020},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {http://arxiv.org/abs/1409.0473},
   year = {2014},
}
@misc{,
   author = {Yann Lecun and L Eon Bottou and Yoshua Bengio and Patrick Haaner Abstract|},
   title = {Gradient-Based Learning Applied to Document Recognition},
}
@article{Levine2020,
   abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
   author = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
   month = {5},
   title = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
   url = {http://arxiv.org/abs/2005.01643},
   year = {2020},
}
@article{,
   abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
   author = {Gabriel Dulac-Arnold and Nir Levine and Daniel J. Mankowitz and Jerry Li and Cosmin Paduraru and Sven Gowal and Todd Hester},
   doi = {10.1007/s10994-021-05961-4},
   issn = {15730565},
   issue = {9},
   journal = {Machine Learning},
   keywords = {Applied reinforcement learning,Real-world,Reinforcement learning},
   month = {9},
   pages = {2419-2468},
   publisher = {Springer},
   title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
   volume = {110},
   year = {2021},
}
@article{Ba2016,
   abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
   author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   month = {7},
   title = {Layer Normalization},
   url = {http://arxiv.org/abs/1607.06450},
   year = {2016},
}
@article{Xiong2020,
   abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
   author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
   month = {2},
   title = {On Layer Normalization in the Transformer Architecture},
   url = {http://arxiv.org/abs/2002.04745},
   year = {2020},
}
@article{Luo2023,
   abstract = {Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.},
   author = {Yicheng Luo and Jackie Kay and Edward Grefenstette and Marc Peter Deisenroth},
   month = {3},
   title = {Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
   url = {http://arxiv.org/abs/2303.17396},
   year = {2023},
}
@article{,
   abstract = {Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.},
   author = {Krishna Teja Chitty-Venkata and Sparsh Mittal and Murali Emani and Venkatram Vishwanath and Arun K. Somani},
   month = {7},
   title = {A Survey of Techniques for Optimizing Transformer Inference},
   url = {http://arxiv.org/abs/2307.07982},
   year = {2023},
}
@article{Chen2023,
   abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf\{Lion\}$ ($\textit\{Evo$\textbf\{L\}$ved S$\textbf\{i\}$gn M$\textbf\{o\}$me$\textbf\{n\}$tum\}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\textit\{zero-shot\}$ and 91.1% $\textit\{fine-tuning\}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.},
   author = {Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
   month = {2},
   title = {Symbolic Discovery of Optimization Algorithms},
   url = {http://arxiv.org/abs/2302.06675},
   year = {2023},
}
@article{Nakamoto2023,
   abstract = {A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that offline RL algorithms that learn such calibrated value functions lead to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL) for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL},
   author = {Mitsuhiko Nakamoto and Yuexiang Zhai and Anikait Singh and Max Sobol Mark and Yi Ma and Chelsea Finn and Aviral Kumar and Sergey Levine},
   month = {3},
   title = {Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning},
   url = {http://arxiv.org/abs/2303.05479},
   year = {2023},
}
@article{Vinyals2019,
   abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
   author = {Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Michaël Mathieu and Andrew Dudzik and Junyoung Chung and David H. Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and Laurent Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander S. Vezhnevets and Rémi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom L. Paine and Caglar Gulcehre and Ziyu Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario Wünsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
   doi = {10.1038/s41586-019-1724-z},
   issn = {14764687},
   issue = {7782},
   journal = {Nature},
   month = {11},
   pages = {350-354},
   pmid = {31666705},
   publisher = {Nature Publishing Group},
   title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
   volume = {575},
   year = {2019},
}
@misc{Huang2020,
   abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are twofold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification , that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 atten-tion/MLP blocks) without difficulty. Code for this work is available here: https://github. com/layer6ai-labs/T-Fixup.},
   author = {Xiao Shi Huang and Felipe Pérez and Jimmy Ba and Maksims Volkovs},
   title = {Improving Transformer Optimization Through Better Initialization},
   url = {https://github.},
   year = {2020},
}
@misc{,
   author = {Aston Zhang and Zachary C Lipton and M U Li and Alexander J Smola},
   title = {Dive into Deep Learning},
}
