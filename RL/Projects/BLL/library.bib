@book{Bellman1972,
   abstract = {"A Rand Corporation research study."},
   author = {Richard Bellman},
   isbn = {069107951X},
   pages = {342},
   publisher = {Princeton University Press},
   title = {Dynamic programming.},
   year = {1972},
}
@book{,
   abstract = {Second edition. "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms"--Provided by publisher. Introduction. Reinforcement Learning -- Examples -- Elements of Reinforcement Learning -- Limitations and Scope -- An Extended Example: Tic-Tac-Toe -- Summary -- Early History of Reinforcement Learning -- Tabular Solution Methods. Multi-armed Bandits. A k-armed Bandit Problem -- Action-value Methods -- The 10-armed Testbed -- Incremental Implementation -- Tracking a Nonstationary Problem -- Optimistic Initial Values -- Upper-Confidence-Bound Action Selection -- Gradient Bandit Algorithms -- Associative Search (Contextual Bandits) -- Summary -- Finite Markov Decision Processes -- The Agent-Environment Interface -- Goals and Rewards -- Returns and Episodes -- Unified Notation for Episodic and Continuing Tasks -- Policies and Value Functions -- Optimal Policies and Optimal Value Functions -- Optimality and Approximation -- Summary -- Dynamic Programming. Policy Evaluation (Prediction) -- Policy Improvement -- Policy Iteration -- Value Iteration -- Asynchronous Dynamic Programming -- Generalized Policy Iteration -- Efficiency of Dynamic Programming -- Summary -- Monte Carlo Methods. Monte Carlo Prediction -- Monte Carlo Estimation of Action Values -- Monte Carlo Control -- Monte Carlo Control without Exploring Starts -- Off-policy Prediction via Importance Sampling -- Incremental Implementation -- Off-policy Monte Carlo Control -- *Discounting-aware Importance Sampling -- *Per-decision Importance Sampling -- Summary -- Temporal-Difference Learning. TD Prediction -- Advantages of TD Prediction Methods -- Optimality of TD(0) -- Sarsa: On-policy TD Control -- Q-learning: Off-policy TD Control -- Expected Sarsa -- Maximization Bias and Double Learning Games, Afterstates, and Other Special Cases -- Summary -- n-step Bootstrapping. n-step TD Prediction -- n-step Sarsa -- n-step Off-policy Learning -- *Per-decision Methods with Control Variates -- Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm -- *A Unifying Algorithm: n-step Q(u) -- Summary -- Planning and Learning with Tabular Methods. Models and Planning -- Dyna: Integrated Planning, Acting, and Learning -- When the Model Is Wrong -- Prioritized Sweeping -- Expected vs. Sample Updates -- Trajectory Sampling -- Real-time Dynamic Programming -- Planning at Decision Time -- Heuristic Search -- Rollout Algorithms -- Monte Carlo Tree Search -- Summary of the Chapter -- Summary of Part I: Dimensions -- Approximate Solution Methods. On-policy Prediction with Approximation. Value-function Approximation -- The Prediction Objective (VE) Stochastic-gradient and Semi-gradient Methods -- Linear Methods -- Feature Construction for Linear Methods -- Polynomials -- Fourier Basis -- Coarse Coding -- Tile Coding -- Radial Basis Functions -- Selecting Step-Size Parameters Manually -- Nonlinear Function Approximation: Artificial Neural Networks -- Least-Squares TD -- Memory-based Function Approximation -- Kernel-based Function Approximation -- Looking Deeper at On-policy Learning: Interest and Emphasis -- Summary -- On-policy Control with Approximation. Episodic Semi-gradient Control -- Semi-gradient n-step Sarsa -- Average Reward: A New Problem Setting for Continuing Tasks -- Deprecating the Discounted Setting -- Differential Semi-gradient n-step Sarsa -- Summary -- *Off-policy Methods with Approximation. Semi-gradient Methods -- Examples of Off-policy Divergence The Deadly Triad -- Linear Value-function Geometry -- Gradient Descent in the Bellman Error -- The Bellman Error is Not Learnable -- Gradient-TD Methods -- Emphatic-TD Methods -- Reducing Variance -- Summary -- Eligibility Traces. The A-return -- TD(A) -- n-step Truncated A-return Methods -- Redoing Updates: Online A-return Algorithm -- True Online TD(A) -- *Dutch Traces in Monte Carlo Learning -- Sarsa(A) -- Variable A and ry -- Off-policy Traces with Control Variates -- Watkins's Q(A) to Tree-Backup(A) -- Stable Off-policy Methods with Traces -- Implementation Issues -- Conclusions -- Policy Gradient Methods. Policy Approximation and its Advantages -- The Policy Gradient Theorem -- REINFORCE: Monte Carlo Policy Gradient -- REINFORCE with Baseline -- Actor-Critic Methods Policy Gradient for Continuing Problems -- Policy Parameterization for Continuous Actions -- Summary -- Looking Deeper. Psychology. Prediction and Control -- Classical Conditioning -- Blocking and Higher-order Conditioning -- The Rescorla-Wagner Model -- The TD Model -- TD Model Simulations -- Instrumental Conditioning -- Delayed Reinforcement -- Cognitive Maps -- Habitual and Goal-directed Behavior -- Summary -- Neuroscience -- Neuroscience Basics -- Reward Signals, Reinforcement Signals, Values, and Prediction Errors -- The Reward Prediction Error Hypothesis -- Dopamine -- Experimental Support for the Reward Prediction Error Hypothesis -- TD Error/Dopamine Correspondence -- Neural Actor-Critic -- Actor and Critic Learning Rules -- Hedonistic Neurons -- Collective Reinforcement Learning -- Model-based Methods in the Brain Addiction -- Summary -- Applications and Case Studies. TD-Gammon -- Samuel's Checkers Player -- Watson's Daily-Double Wagering -- Optimizing Memory Control -- Human-level Video Game Play -- Mastering the Game of Go -- AlphaGo -- AlphaGo Zero -- Personalized Web Services -- Thermal Soaring -- Frontiers. General Value Functions and Auxiliary Tasks -- Temporal Abstraction via Options -- Observations and State -- Designing Reward Signals -- Remaining Issues -- Experimental Support for the Reward Prediction Error Hypothesis.},
   author = {Richard S. Sutton and Andrew G. Barto},
   isbn = {9780262039246},
   pages = {526},
   title = {Reinforcement learning : an introduction},
}
@misc{,
   abstract = {B e n j a m i n B l a n k e r t z u n d V e r a R ö h r A l g o r i t h m e n u n d D a t e n s t r u k t u r e n},
   author = {Benjamin Blankertz and Vera Röhr},
   title = {Algorithmen und Datenstrukturen},
}
@article{Nakamoto2023,
   abstract = {A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that offline RL algorithms that learn such calibrated value functions lead to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL) for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL},
   author = {Mitsuhiko Nakamoto and Yuexiang Zhai and Anikait Singh and Max Sobol Mark and Yi Ma and Chelsea Finn and Aviral Kumar and Sergey Levine},
   month = {3},
   title = {Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning},
   url = {http://arxiv.org/abs/2303.05479},
   year = {2023},
}
@article{Luo2023,
   abstract = {Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.},
   author = {Yicheng Luo and Jackie Kay and Edward Grefenstette and Marc Peter Deisenroth},
   month = {3},
   title = {Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
   url = {http://arxiv.org/abs/2303.17396},
   year = {2023},
}
@article{Levine2020,
   abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
   author = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
   month = {5},
   title = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
   url = {http://arxiv.org/abs/2005.01643},
   year = {2020},
}
@article{Zhuang2023,
   abstract = {Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to the overestimation of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we get a surprising finding that some online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark indicate this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.},
   author = {Zifeng Zhuang and Kun Lei and Jinxin Liu and Donglin Wang and Yilang Guo},
   month = {2},
   title = {Behavior Proximal Policy Optimization},
   url = {http://arxiv.org/abs/2302.11312},
   year = {2023},
}
@misc{,
   author = {Sergey Levine},
   title = {Policy Gradients CS 285},
}
@article{Zheng2022,
   abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
   author = {Qinqing Zheng and Amy Zhang and Aditya Grover},
   month = {2},
   title = {Online Decision Transformer},
   url = {http://arxiv.org/abs/2202.05607},
   year = {2022},
}
@article{Li2023,
   abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
   author = {Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
   month = {1},
   title = {A Survey on Transformers in Reinforcement Learning},
   url = {http://arxiv.org/abs/2301.03044},
   year = {2023},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {6},
   title = {Attention Is All You Need},
   url = {http://arxiv.org/abs/1706.03762},
   year = {2017},
}
@article{Mathieu2023,
   abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.},
   author = {Michaël Mathieu and Sherjil Ozair and Srivatsan Srinivasan and Caglar Gulcehre and Shangtong Zhang and Ray Jiang and Tom Le Paine and Richard Powell and Konrad Żołna and Julian Schrittwieser and David Choi and Petko Georgiev and Daniel Toyama and Aja Huang and Roman Ring and Igor Babuschkin and Timo Ewalds and Mahyar Bordbar and Sarah Henderson and Sergio Gómez Colmenarejo and Aäron van den Oord and Wojciech Marian Czarnecki and Nando de Freitas and Oriol Vinyals},
   month = {8},
   title = {AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning},
   url = {http://arxiv.org/abs/2308.03526},
   year = {2023},
}
@misc{,
   author = {Aston Zhang and Zachary C Lipton and M U Li and Alexander J Smola},
   title = {Dive into Deep Learning},
}
@misc{,
   author = {Yann Lecun and L Eon Bottou and Yoshua Bengio and Patrick Haaner Abstract|},
   title = {Gradient-Based Learning Applied to Document Recognition},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {http://arxiv.org/abs/1409.0473},
   year = {2014},
}
@article{Ba2016,
   abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
   author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   month = {7},
   title = {Layer Normalization},
   url = {http://arxiv.org/abs/1607.06450},
   year = {2016},
}
@article{Chen2021,
   abstract = {We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.},
   author = {Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
   month = {6},
   title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
   url = {http://arxiv.org/abs/2106.01345},
   year = {2021},
}
@article{Zheng2022,
   abstract = {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.},
   author = {Qinqing Zheng and Amy Zhang and Aditya Grover},
   month = {2},
   title = {Online Decision Transformer},
   url = {http://arxiv.org/abs/2202.05607},
   year = {2022},
}
@article{Zhang2023,
   abstract = {Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.},
   author = {Tianjun Zhang and Fangchen Liu and Justin Wong and Pieter Abbeel and Joseph E. Gonzalez},
   month = {2},
   title = {The Wisdom of Hindsight Makes Language Models Better Instruction Followers},
   url = {http://arxiv.org/abs/2302.05206},
   year = {2023},
}
@article{Vaswani2017,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {6},
   title = {Attention Is All You Need},
   url = {http://arxiv.org/abs/1706.03762},
   year = {2017},
}
@article{Prudencio2023,
   abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks&#x2019; properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
   author = {Rafael Figueiredo Prudencio and Marcos R.O.A. Maximo and Esther Luna Colombini},
   doi = {10.1109/TNNLS.2023.3250269},
   issn = {21622388},
   journal = {IEEE Transactions on Neural Networks and Learning Systems},
   keywords = {Batch reinforcement learning (RL),Behavioral sciences,Benchmark testing,Games,Markov processes,RL,Taxonomy,Training,Uncertainty,deep learning (DL),offline RL},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems},
   year = {2023},
}
@article{Mathieu2023,
   abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.},
   author = {Michaël Mathieu and Sherjil Ozair and Srivatsan Srinivasan and Caglar Gulcehre and Shangtong Zhang and Ray Jiang and Tom Le Paine and Richard Powell and Konrad Żołna and Julian Schrittwieser and David Choi and Petko Georgiev and Daniel Toyama and Aja Huang and Roman Ring and Igor Babuschkin and Timo Ewalds and Mahyar Bordbar and Sarah Henderson and Sergio Gómez Colmenarejo and Aäron van den Oord and Wojciech Marian Czarnecki and Nando de Freitas and Oriol Vinyals},
   month = {8},
   title = {AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning},
   url = {http://arxiv.org/abs/2308.03526},
   year = {2023},
}
@article{Li2023,
   abstract = {Transformer has been considered the dominating neural architecture in NLP and CV, mostly under supervised settings. Recently, a similar surge of using Transformers has appeared in the domain of reinforcement learning (RL), but it is faced with unique design choices and challenges brought by the nature of RL. However, the evolution of Transformers in RL has not yet been well unraveled. In this paper, we seek to systematically review motivations and progress on using Transformers in RL, provide a taxonomy on existing works, discuss each sub-field, and summarize future prospects.},
   author = {Wenzhe Li and Hao Luo and Zichuan Lin and Chongjie Zhang and Zongqing Lu and Deheng Ye},
   month = {1},
   title = {A Survey on Transformers in Reinforcement Learning},
   url = {http://arxiv.org/abs/2301.03044},
   year = {2023},
}
@article{Tay2022,
   abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision, and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former"models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this article characterizes a large and thoughtful selection of recent efficiency-flavored "X-former"models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
   author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
   doi = {10.1145/3530811},
   issn = {15577341},
   issue = {6},
   journal = {ACM Computing Surveys},
   keywords = {Transformers,attention,deep learning,neural networks},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Efficient Transformers: A Survey},
   volume = {55},
   year = {2022},
}
@article{Zhuang2023,
   abstract = {Recent advances in Transformers have come with a huge requirement on computing resources, highlighting the importance of developing efficient training techniques to make Transformer training faster, at lower cost, and to higher accuracy by the efficient use of computation and memory resources. This survey provides the first systematic overview of the efficient training of Transformers, covering the recent progress in acceleration arithmetic and hardware, with a focus on the former. We analyze and compare methods that save computation and memory costs for intermediate tensors during training, together with techniques on hardware/algorithm co-design. We finally discuss challenges and promising areas for future research.},
   author = {Bohan Zhuang and Jing Liu and Zizheng Pan and Haoyu He and Yuetian Weng and Chunhua Shen},
   month = {2},
   title = {A Survey on Efficient Training of Transformers},
   url = {http://arxiv.org/abs/2302.01107},
   year = {2023},
}
@article{Zhuang2023,
   abstract = {Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to the overestimation of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we get a surprising finding that some online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark indicate this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.},
   author = {Zifeng Zhuang and Kun Lei and Jinxin Liu and Donglin Wang and Yilang Guo},
   month = {2},
   title = {Behavior Proximal Policy Optimization},
   url = {http://arxiv.org/abs/2302.11312},
   year = {2023},
}
@misc{,
   author = {Sergey Levine},
   title = {Policy Gradients CS 285},
}
@misc{,
   title = {An Evaluation of Transformer Variants | dalle-mini-Weights & Biases https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants-VmlldzoxNjk4MTIw An Evaluation of Transformer Variants Training of different transformer variants for text-to-image generation with DALL-E-mini},
   url = {https://wandb.ai/dalle-mini/dalle-mini/reports/An-Evaluation-of-Transformer-Variants--VmlldzoxNjk4MTIw},
}
@article{Dosovitskiy2020,
   abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
   author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
   month = {10},
   title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
   url = {http://arxiv.org/abs/2010.11929},
   year = {2020},
}
@article{Bahdanau2014,
   abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
   author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
   month = {9},
   title = {Neural Machine Translation by Jointly Learning to Align and Translate},
   url = {http://arxiv.org/abs/1409.0473},
   year = {2014},
}
@misc{,
   author = {Yann Lecun and L Eon Bottou and Yoshua Bengio and Patrick Haaner Abstract|},
   title = {Gradient-Based Learning Applied to Document Recognition},
}
@article{Levine2020,
   abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
   author = {Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
   month = {5},
   title = {Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
   url = {http://arxiv.org/abs/2005.01643},
   year = {2020},
}
@article{,
   abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
   author = {Gabriel Dulac-Arnold and Nir Levine and Daniel J. Mankowitz and Jerry Li and Cosmin Paduraru and Sven Gowal and Todd Hester},
   doi = {10.1007/s10994-021-05961-4},
   issn = {15730565},
   issue = {9},
   journal = {Machine Learning},
   keywords = {Applied reinforcement learning,Real-world,Reinforcement learning},
   month = {9},
   pages = {2419-2468},
   publisher = {Springer},
   title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
   volume = {110},
   year = {2021},
}
@article{Xiong2020,
   abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
   author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
   month = {2},
   title = {On Layer Normalization in the Transformer Architecture},
   url = {http://arxiv.org/abs/2002.04745},
   year = {2020},
}
@article{Ba2016,
   abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
   author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
   month = {7},
   title = {Layer Normalization},
   url = {http://arxiv.org/abs/1607.06450},
   year = {2016},
}
@article{Luo2023,
   abstract = {Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.},
   author = {Yicheng Luo and Jackie Kay and Edward Grefenstette and Marc Peter Deisenroth},
   month = {3},
   title = {Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions},
   url = {http://arxiv.org/abs/2303.17396},
   year = {2023},
}
@article{,
   abstract = {Recent years have seen a phenomenal rise in performance and applications of transformer neural networks. The family of transformer networks, including Bidirectional Encoder Representations from Transformer (BERT), Generative Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their effectiveness across Natural Language Processing (NLP) and Computer Vision (CV) domains. Transformer-based networks such as ChatGPT have impacted the lives of common men. However, the quest for high predictive performance has led to an exponential increase in transformers' memory and compute footprint. Researchers have proposed techniques to optimize transformer inference at all levels of abstraction. This paper presents a comprehensive survey of techniques for optimizing the inference phase of transformer networks. We survey techniques such as knowledge distillation, pruning, quantization, neural architecture search and lightweight network design at the algorithmic level. We further review hardware-level optimization techniques and the design of novel hardware accelerators for transformers. We summarize the quantitative results on the number of parameters/FLOPs and accuracy of several models/techniques to showcase the tradeoff exercised by them. We also outline future directions in this rapidly evolving field of research. We believe that this survey will educate both novice and seasoned researchers and also spark a plethora of research efforts in this field.},
   author = {Krishna Teja Chitty-Venkata and Sparsh Mittal and Murali Emani and Venkatram Vishwanath and Arun K. Somani},
   month = {7},
   title = {A Survey of Techniques for Optimizing Transformer Inference},
   url = {http://arxiv.org/abs/2307.07982},
   year = {2023},
}
@article{Chen2023,
   abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf\{Lion\}$ ($\textit\{Evo$\textbf\{L\}$ved S$\textbf\{i\}$gn M$\textbf\{o\}$me$\textbf\{n\}$tum\}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\textit\{zero-shot\}$ and 91.1% $\textit\{fine-tuning\}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.},
   author = {Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
   month = {2},
   title = {Symbolic Discovery of Optimization Algorithms},
   url = {http://arxiv.org/abs/2302.06675},
   year = {2023},
}
@article{Nakamoto2023,
   abstract = {A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also being calibrated, in the sense that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply be the behavior policy. We show that offline RL algorithms that learn such calibrated value functions lead to effective online fine-tuning, enabling us to take the benefits of offline initializations in online fine-tuning. In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL) for offline RL within a one-line code change. Empirically, Cal-QL outperforms state-of-the-art methods on 9/11 fine-tuning benchmark tasks that we study in this paper. Code and video are available at https://nakamotoo.github.io/Cal-QL},
   author = {Mitsuhiko Nakamoto and Yuexiang Zhai and Anikait Singh and Max Sobol Mark and Yi Ma and Chelsea Finn and Aviral Kumar and Sergey Levine},
   month = {3},
   title = {Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning},
   url = {http://arxiv.org/abs/2303.05479},
   year = {2023},
}
@article{Vinyals2019,
   abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
   author = {Oriol Vinyals and Igor Babuschkin and Wojciech M. Czarnecki and Michaël Mathieu and Andrew Dudzik and Junyoung Chung and David H. Choi and Richard Powell and Timo Ewalds and Petko Georgiev and Junhyuk Oh and Dan Horgan and Manuel Kroiss and Ivo Danihelka and Aja Huang and Laurent Sifre and Trevor Cai and John P. Agapiou and Max Jaderberg and Alexander S. Vezhnevets and Rémi Leblond and Tobias Pohlen and Valentin Dalibard and David Budden and Yury Sulsky and James Molloy and Tom L. Paine and Caglar Gulcehre and Ziyu Wang and Tobias Pfaff and Yuhuai Wu and Roman Ring and Dani Yogatama and Dario Wünsch and Katrina McKinney and Oliver Smith and Tom Schaul and Timothy Lillicrap and Koray Kavukcuoglu and Demis Hassabis and Chris Apps and David Silver},
   doi = {10.1038/s41586-019-1724-z},
   issn = {14764687},
   issue = {7782},
   journal = {Nature},
   month = {11},
   pages = {350-354},
   pmid = {31666705},
   publisher = {Nature Publishing Group},
   title = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
   volume = {575},
   year = {2019},
}
@misc{Huang2020,
   abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are twofold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification , that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 atten-tion/MLP blocks) without difficulty. Code for this work is available here: https://github. com/layer6ai-labs/T-Fixup.},
   author = {Xiao Shi Huang and Felipe Pérez and Jimmy Ba and Maksims Volkovs},
   title = {Improving Transformer Optimization Through Better Initialization},
   url = {https://github.},
   year = {2020},
}
@misc{,
   author = {Aston Zhang and Zachary C Lipton and M U Li and Alexander J Smola},
   title = {Dive into Deep Learning},
}
@article{Silver2017,
   abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
   author = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
   month = {12},
   title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
   url = {http://arxiv.org/abs/1712.01815},
   year = {2017},
}
@article{Agarwal2019,
   abstract = {Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.},
   author = {Rishabh Agarwal and Dale Schuurmans and Mohammad Norouzi},
   month = {7},
   title = {An Optimistic Perspective on Offline Reinforcement Learning},
   url = {http://arxiv.org/abs/1907.04543},
   year = {2019},
}
@article{Liu2023,
   abstract = {This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and GPT-4) research, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT-related research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
   author = {Yiheng Liu and Tianle Han and Siyuan Ma and Jiayue Zhang and Yuanyuan Yang and Jiaming Tian and Hao He and Antong Li and Mengshen He and Zhengliang Liu and Zihao Wu and Lin Zhao and Dajiang Zhu and Xiang Li and Ning Qiang and Dingang Shen and Tianming Liu and Bao Ge},
   doi = {10.1016/j.metrad.2023.100017},
   month = {4},
   title = {Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models},
   url = {http://arxiv.org/abs/2304.01852 http://dx.doi.org/10.1016/j.metrad.2023.100017},
   year = {2023},
}
@article{Wang2023,
   abstract = {When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.},
   author = {Letian Wang and Jie Liu and Hao Shao and Wenshuo Wang and Ruobing Chen and Yu Liu and Steven L. Waslander},
   month = {5},
   title = {Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors},
   url = {http://arxiv.org/abs/2305.04412},
   year = {2023},
}
@article{Popel2018,
   abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra “more data and larger models”, we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
   author = {Martin Popel and Ondřej Bojar},
   doi = {10.2478/pralin-2018-0002},
   issue = {1},
   journal = {The Prague Bulletin of Mathematical Linguistics},
   month = {5},
   pages = {43-70},
   publisher = {Charles University in Prague, Karolinum Press},
   title = {Training Tips for the Transformer Model},
   volume = {110},
   year = {2018},
}
@article{Silver2017,
   abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
   author = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
   month = {12},
   title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
   url = {http://arxiv.org/abs/1712.01815},
   year = {2017},
}
@article{Xiong2020,
   abstract = {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.},
   author = {Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
   month = {2},
   title = {On Layer Normalization in the Transformer Architecture},
   url = {http://arxiv.org/abs/2002.04745},
   year = {2020},
}
@article{Kostrikov2021,
   abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
   author = {Ilya Kostrikov and Ashvin Nair and Sergey Levine},
   month = {10},
   title = {Offline Reinforcement Learning with Implicit Q-Learning},
   url = {http://arxiv.org/abs/2110.06169},
   year = {2021},
}
@article{Sherstinsky2018,
   abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this tutorial valuable as well.},
   author = {Alex Sherstinsky},
   doi = {10.1016/j.physd.2019.132306},
   month = {8},
   title = {Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network},
   url = {http://arxiv.org/abs/1808.03314 http://dx.doi.org/10.1016/j.physd.2019.132306},
   year = {2018},
}
@article{Khan2021,
   abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
   author = {Salman Khan and Muzammal Naseer and Munawar Hayat and Syed Waqas Zamir and Fahad Shahbaz Khan and Mubarak Shah},
   doi = {10.1145/3505244},
   month = {1},
   title = {Transformers in Vision: A Survey},
   url = {http://arxiv.org/abs/2101.01169 http://dx.doi.org/10.1145/3505244},
   year = {2021},
}
@article{Kostrikov2021,
   abstract = {Offline reinforcement learning requires reconciling two conflicting aims: learning a policy that improves over the behavior policy that collected the dataset, while at the same time minimizing the deviation from the behavior policy so as to avoid errors due to distributional shift. This trade-off is critical, because most current offline reinforcement learning methods need to query the value of unseen actions during training to improve the policy, and therefore need to either constrain these actions to be in-distribution, or else regularize their values. We propose an offline RL method that never needs to evaluate actions outside of the dataset, but still enables the learned policy to improve substantially over the best behavior in the data through generalization. The main insight in our work is that, instead of evaluating unseen actions from the latest policy, we can approximate the policy improvement step implicitly by treating the state value function as a random variable, with randomness determined by the action (while still integrating over the dynamics to avoid excessive optimism), and then taking a state conditional upper expectile of this random variable to estimate the value of the best actions in that state. This leverages the generalization capacity of the function approximator to estimate the value of the best available action at a given state without ever directly querying a Q-function with this unseen action. Our algorithm alternates between fitting this upper expectile value function and backing it up into a Q-function. Then, we extract the policy via advantage-weighted behavioral cloning. We dub our method implicit Q-learning (IQL). IQL demonstrates the state-of-the-art performance on D4RL, a standard benchmark for offline reinforcement learning. We also demonstrate that IQL achieves strong performance fine-tuning using online interaction after offline initialization.},
   author = {Ilya Kostrikov and Ashvin Nair and Sergey Levine},
   month = {10},
   title = {Offline Reinforcement Learning with Implicit Q-Learning},
   url = {http://arxiv.org/abs/2110.06169},
   year = {2021},
}
@article{Hendrycks2016,
   abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf\{1\}_\{x>0\}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
   author = {Dan Hendrycks and Kevin Gimpel},
   month = {6},
   title = {Gaussian Error Linear Units (GELUs)},
   url = {http://arxiv.org/abs/1606.08415},
   year = {2016},
}
@article{Mnih2013,
   abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   month = {12},
   title = {Playing Atari with Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/1312.5602},
   year = {2013},
}
@article{Fawzi2022,
   abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems—from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4 × 4 matrices in a finite field, where AlphaTensor’s algorithm improves on Strassen’s two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor’s ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
   author = {Alhussein Fawzi and Matej Balog and Aja Huang and Thomas Hubert and Bernardino Romera-Paredes and Mohammadamin Barekatain and Alexander Novikov and Francisco J. Francisco and Julian Schrittwieser and Grzegorz Swirszcz and David Silver and Demis Hassabis and Pushmeet Kohli},
   doi = {10.1038/s41586-022-05172-4},
   issn = {14764687},
   issue = {7930},
   journal = {Nature},
   month = {10},
   pages = {47-53},
   pmid = {36198780},
   publisher = {Nature Research},
   title = {Discovering faster matrix multiplication algorithms with reinforcement learning},
   volume = {610},
   year = {2022},
}
@article{Wang2023,
   abstract = {When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.},
   author = {Letian Wang and Jie Liu and Hao Shao and Wenshuo Wang and Ruobing Chen and Yu Liu and Steven L. Waslander},
   month = {5},
   title = {Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors},
   url = {http://arxiv.org/abs/2305.04412},
   year = {2023},
}
@article{,
   abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.},
   author = {Keiron O'Shea and Ryan Nash},
   month = {11},
   title = {An Introduction to Convolutional Neural Networks},
   url = {http://arxiv.org/abs/1511.08458},
   year = {2015},
}
@misc{,
   abstract = {The Generative Pre-trained Transformer (GPT) represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.},
   author = {Gokul Yenduri and Chemmalar G Selvi and Gautam Srivastava and Praveen Kumar Reddy Maddikunta and Deepti G Raj and Rutvij H Jhaveri and Weizheng Wang and Athanasios V Vasilakos and Thippa Reddy Gadekallu},
   keywords = {Artificial Intelligence,Index Terms-Generative Pre-trained Transformer,Natural language processing},
   title = {GPT (Generative Pre-trained Transformer)-A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions},
}
