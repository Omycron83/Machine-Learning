Testing my NN data (100 iterations each, usually taking around 22secs in theta1 = 19x785 etc. setup)
----Test 1:----
alpha = 1
_lambda = 0
theta1 = 4x785
theta2 = 7x5
theta3 = 10x8
Training error: 2.701
Test error: 2.704
Training accuracy: 0.396
Test accuracy: 0.379

----Test 2:----
alpha = 3
_lambda = 1
theta1 = 4x785
theta2 = 7x5
theta3 = 10x8
Training error: 2.558
Test error: 3.315
Training accuracy: 0.389
Test accuracy: 0.177

----Test 3:----
alpha = 3
_lambda = 0.3
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 1.718
Test error: 1.754
Training accuracy: 0.752
Test accuracy: 0.745
(Managed to get that, after 1000 iterations, to:
Training error:  0.305
Test error: 0.619
Training accuracy: 0.956
Test accuracy: 0.911
)

----Test 4:----
alpha = 3
_lambda = 2
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 1.932
Test error: 2.119
Training accuracy: 0.633
Test accuracy: 0.510

----Test 4:----
alpha = 10/5/5
_lambda = 0.3
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 2.584
Test error: 2.577
Training accuracy: 0.412
Test accuracy: 0.432

----Test 5:----
alpha = 0.5
_lambda = 0.3
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 2.25
Test error: 2.25
Training accuracy: 0.650
Test accuracy: 0.645

----Test 6:----
alpha = 0.1
_lambda = 0.3
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 3.161
Test error: 3.127
Training accuracy: 0.331
Test accuracy: 0.464
(Just very slow at converging, kinda sucked so ill probably stay with around 3)

---Using ReLU instead of sigmoid everywhere except output (100 iterations and MSE instead of logistic cost):---
alpha = 0.1
_lambda = 0.3
theta1 = 4x785
theta2 = 7x5
theta3 = 10x8
Training error: 0.586
Test error: 0.940
Training accuracy: 0.555
Test accuracy: 0.451

alpha = 0.15
_lambda = 0.3
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 0.230
Test error: 0.482
Training accuracy: 0.857
Test accuracy: 0.755
(29secs pretty representative of speed (slower than sigmoid?), also shut down the 0.3 attempt due to it just not converging after a couple of iterations)

alpha = 0.2
_lambda = 1
theta1 = 19x785
theta2 = 19x20
theta3 = 10x20
Training error: 0.237
Test error: 0.424
Training accuracy: 0.850
Test accuracy: 0.778

alpha = 0.2
_lambda = 1
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.133
Test error: 0.281
Training accuracy: 0.917
Test accuracy: 0.854

So currently trying out Batch Gradient descent:
alpha = 0.15
_lambda = 0
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.003
Test error: 0.200
Training accuracy: 0.9995
Test accuracy: 0.877
Batchsize = 1000
So very POG but not really great at getting a high accuracy on testing/generalising.

#From now on using logistic regression cost, so those numbers will differ but be more comparable to the other model

alpha = 0.15
_lambda = 0.9
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.111
Test error: 1.659
Training accuracy: 0.987
Test accuracy: 0.929
Batchsize = 1000

alpha = 0.15
_lambda = 1.2
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.148
Test error: 1.647
Training accuracy: 0.980
Test accuracy: 0.930
Batchsize = 1000

alpha = 0.15
_lambda = 5
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.201
Test error: 1.464
Training accuracy: 0.974
Test accuracy: 0.939
Batchsize = 1000

alpha = 0.15
_lambda = 3
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.142
Test error: 1.527
Training accuracy: 0.983
Test accuracy: 0.935
Batchsize = 1000

alpha = 0.15
_lambda = 10
theta1 = 39x785
theta2 = 39x40
theta3 = 10x40
Training error: 0.248
Test error: 1.425
Training accuracy: 0.970
Test accuracy: 0.941
Batchsize = 1000

alpha = 0.15
_lambda = 5
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.180
Test error: 1.345
Training accuracy: 0.978
Test accuracy: 0.944
Batchsize = 1000

alpha = 0.15
_lambda = 10
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.268
Test error: 1.603
Training accuracy: 0.965
Test accuracy: 0.935
Batchsize = 1000

#the only way i see to avoid overfitting rn is to build another model with only one hidden layer:
ok turns out that solution isnt actually better:
It'll reduce training accuracy and increase training error as well as doing the same with testing (getting to 0.887 or smth on testing with a bunch of iterations)
Yeah so that didnt work out and since i dont want to do manual testing anymore im currently doing grid search for lambda, alpha and batch size, cya when that shits done (probably takes like a day or smth)
What i "figured out" through mf grid search:
optimal lambda = 0.2
optimal alpha = 0.21
Ill stick with the alpha but may increase lambda:

alpha = 0.21
_lambda = 0.4
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.110 #regularized
Test error: 1.377
Training accuracy: 0.985
Test accuracy: 0.942
Batchsize = 1000

alpha = 0.21
_lambda = 0.5
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.065 
Test error: 1.886
Training accuracy:0.993
Test accuracy: 0.922
Batchsize = 1000

alpha = 0.21
_lambda = 6
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.171 
Test error: 1.280
Training accuracy: 0.979	
Test accuracy: 0.943
Batchsize = 1000 #after 150 iterations

alpha = 0.21
_lambda = 13
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.269 
Test error: 1.787
Training accuracy: 0.967		
Test accuracy: 0.922
Batchsize = 1000 #after 150 iterations

alpha = 0.21
_lambda = 6
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error: 0.167
Test error: 1.654
Training accuracy: 0.980	
Test accuracy: 0.929
Batchsize = 1000 #after "50 iterations

alpha = 0.21
_lambda = 7
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error:  0.210
Test error: 1.287
Training accuracy: 0.972	
Test accuracy: 0.945
Batchsize = 1000 #after "50 iterations

alpha = 0.21
_lambda = 8
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error:  0.204
Test error: 1.245
Training accuracy: 0.974	
Test accuracy: 0.947
Batchsize = 1000 #after "50 iterations

alpha = 0.21
_lambda = 10
theta1 = 31x785
theta2 = 31x32
theta3 = 10x32
Training error:  0.224
Test error: 1.426
Training accuracy: 0.972	
Test accuracy: 0.938
Batchsize = 1000 #after "50 iterations








